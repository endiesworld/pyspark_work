{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cfacbb-450b-496f-93f4-abcb38e9f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Replace \"../../../../\" with the actual absolute path to your home directory\n",
    "home_directory = \"../../../\"\n",
    "os.environ[\"SPARK_HOME\"] = os.path.join(home_directory, \"spark-3.3.2-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2acd7d0-56e7-4c56-a125-8986a4efe3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fe3686-f4e0-4b07-9d81-da38db4f37cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/10 16:05:32 WARN Utils: Your hostname, Endiesworld resolves to a loopback address: 127.0.1.1; using 172.22.195.180 instead (on interface eth0)\n",
      "24/03/10 16:05:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/10 16:05:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.22.195.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sql_with_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f16a8058400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('sql_with_spark') \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262b56b0-7453-42ae-96d5-6479d1337b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------+-----+\n",
      "|customerid|order_date|      item|quantity|price|\n",
      "+----------+----------+----------+--------+-----+\n",
      "|     10330| 30-Jun-99|Pogo stick|       1| 28.0|\n",
      "|     10101| 30-Jun-99|      Raft|       1| 58.0|\n",
      "|     10298|  1-Jul-99|Skateboard|       1| 33.0|\n",
      "+----------+----------+----------+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('ITEMS_ORDERED.csv', inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e14053-8eb9-4576-8cff-36cd078da89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerid: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3715d95-54fa-4e02-9c1c-67c85ef11a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endie/Projects/Data_Engineering/Pyspark_work/../../../spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('items_ordered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1cbed3f-af1d-46b4-b49f-39d71b762d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|quantity|max(price)|\n",
      "+--------+----------+\n",
      "|       1|    1250.0|\n",
      "|       3|     14.75|\n",
      "|       4|     125.0|\n",
      "|       2|      88.7|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT quantity, max(price)\n",
    "FROM items_ordered\n",
    "GROUP BY quantity;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcaf6530-9c89-49c6-a494-0f5e40377a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/10 16:29:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: customerid, firstname, lastname, city, state, \n",
      " Schema: customerid, firstname, lastname, city, state, _c5\n",
      "Expected: _c5 but found: \n",
      "CSV file: file:///home/endie/Projects/Data_Engineering/Pyspark_work/customers.csv\n",
      "+----------+---------+--------+----------+----------+----+\n",
      "|customerid|firstname|lastname|      city|     state| _c5|\n",
      "+----------+---------+--------+----------+----------+----+\n",
      "|     10101|     John|    Gray|    Lynden|Washington|null|\n",
      "|     10298|    Leroy|   Brown|   Pinetop|   Arizona|null|\n",
      "|     10299|    Elroy|  Keller|Snoqualmie|Washington|null|\n",
      "+----------+---------+--------+----------+----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('customers.csv', inferSchema=True)\n",
    "df_2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd04173-d1f2-472b-b8cb-4ba534f3ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------+----------+\n",
      "|customerid|firstname|lastname|   city|     state|\n",
      "+----------+---------+--------+-------+----------+\n",
      "|     10101|     John|    Gray| Lynden|Washington|\n",
      "|     10298|    Leroy|   Brown|Pinetop|   Arizona|\n",
      "+----------+---------+--------+-------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['customerid', 'firstname', 'lastname', 'city', 'state']\n",
    "df_2 = df_2.select(columns) \n",
    "df_2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3c7c4d8-6bc0-42e5-900d-4533de54e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.registerTempTable('customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12762b0a-8a3e-42af-aad4-fee6b8577f00",
   "metadata": {},
   "source": [
    "## Review Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a717882-cf86-404f-89f9-c3a878ed32da",
   "metadata": {},
   "source": [
    "*How many people are in each unique state in the customers table? Select the state and display the number of people in each. Hint: count is used to count rows in a column, sum works on numeric data only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49523b6e-2eaf-4bed-ad8d-483d20213495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|state_customer|         state|\n",
      "+--------------+--------------+\n",
      "|             1|        Hawaii|\n",
      "|             2|        Oregon|\n",
      "|             2|    Washington|\n",
      "|             1|North Carolina|\n",
      "|             6|       Arizona|\n",
      "|             1|         Idaho|\n",
      "|             1|South Carolina|\n",
      "|             1|     Wisconsin|\n",
      "|             2|      Colorado|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(customerid) AS state_customer, state\n",
    "FROM customers\n",
    "GROUP BY state\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381aa91-1bf6-4138-8bad-6d713b7d25d9",
   "metadata": {},
   "source": [
    "*From the items_ordered table, select the item, maximum price, and minimum price for each specific item in the table. Hint: The items will need to be broken up into separate groups.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f32e7da-7b5e-445c-b62f-3f1322909f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|max_price|min_price|        item|\n",
      "+---------+---------+------------+\n",
      "|     33.0|     33.0|  Skateboard|\n",
      "|    125.0|    125.0|   Life Vest|\n",
      "|     40.0|     40.0|Canoe paddle|\n",
      "|     28.0|     28.0|  Pogo stick|\n",
      "|     6.75|      4.5|    Umbrella|\n",
      "|    89.22|     88.7|Sleeping Bag|\n",
      "|    380.5|    380.5|     Bicycle|\n",
      "|      8.0|      8.0|     Compass|\n",
      "|     18.3|     18.3|   Rain Coat|\n",
      "|    16.75|    16.75|      Shovel|\n",
      "|     22.0|     22.0|      Helmet|\n",
      "|     58.0|     58.0|        Raft|\n",
      "|     32.0|     32.0|   Lawnchair|\n",
      "|     88.0|    79.99|        Tent|\n",
      "|    280.0|    280.0|       Canoe|\n",
      "|     12.5|     12.5|   Ear Muffs|\n",
      "|     29.0|     16.0|     Lantern|\n",
      "|    192.5|   180.79|    Unicycle|\n",
      "|     28.0|      4.5|  Flashlight|\n",
      "|   1250.0|   1250.0|   Parachute|\n",
      "+---------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT MAX(price) AS max_price, MIN(price) AS min_price, item\n",
    "FROM items_ordered\n",
    "GROUP BY item\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81988716-bd39-4b52-9cca-44e931f75f4b",
   "metadata": {},
   "source": [
    "*How many orders did each customer make? Use the items_ordered table. Select the customerid, number of orders they made, and the sum of their orders. Click the Group By answers link below if you have any problems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a9a75b0-02f8-43f5-8bc8-b6f607b4aed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|customerid|nos_order|total_cost|\n",
      "+----------+---------+----------+\n",
      "|     10449|        6|    930.79|\n",
      "|     10410|        2|    281.72|\n",
      "|     10339|        1|       4.5|\n",
      "|     10101|        6|    320.75|\n",
      "|     10315|        1|       8.0|\n",
      "|     10438|        3|     95.24|\n",
      "|     10330|        3|     72.75|\n",
      "|     10299|        2|    1288.0|\n",
      "|     10413|        1|      32.0|\n",
      "|     10298|        5|    118.88|\n",
      "|     10439|        2|     113.5|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customerid, COUNT(order_date) AS nos_order, SUM(price) AS total_cost\n",
    "FROM items_ordered\n",
    "GROUP BY customerid\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25c8c453-7b4a-41b6-b060-feeb5ab158e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4507ab8-8d08-4989-812b-33eadd2e105c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
